#
# Copyright (c) 2020 Christopher Prevoe
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.
#

""" Backup file rotation script for backup files. See DESCRIPTION."""
import logging
import json
import re
import fnmatch
import os
from os.path import join, getmtime, basename
from functools import partial
from datetime import datetime

LOG = logging.getLogger(__name__)
EXIT_CODE_MISSING_BACKUP_ROOT = 100

class BackupRotationException(Exception):
    """ Base class for handled exceptions generated by this script """
    def __init__(self, message, preferred_exit_code):
        super().__init__(message)
        self.message = message
        self.preferred_exit_code = preferred_exit_code


class BackupRootFolderMissingException(BackupRotationException):
    """ Exception for when the backup root folder is missing """
    def __init__(self, backup_root):
        message = "The backup root \"%s\" does not exist. Please create " + \
            "it and rerun the script."
        super().__init__(message % backup_root, EXIT_CODE_MISSING_BACKUP_ROOT)


class BackupRotator():
    """ A Rotator which creates a plan and effects it. """
    def __init__(self, time_buckets):
        self.is_dry_run = False
        self.backup_root = "backups"
        self.pattern = "*.*"
        self.__time_buckets = \
                sorted(time_buckets.items(),
                       key=lambda x: (datetime.now() + x[1].get("frequency")),
                       reverse=True)

        self.backup_plan = {
        }

    @staticmethod
    def __get_mod_time(filename):
        """ Convenience method to retrieve the last modification time of a
            given file as datetime """
        return datetime.fromtimestamp(getmtime(filename))

    def effect_promotions(self):
        """ Promotes files which are listed in files_to_promote into the
            backup time buckets provided."""
        backup_plan = self.backup_plan
        LOG.debug("Handling promotions")
        for backup_directory in map(lambda x: x[0], self.__time_buckets):
            for filename in backup_plan[backup_directory]["files_to_promote"]:
                LOG.debug("Promoting %s to %s", filename, backup_directory)
                target_filename = join(
                    self.backup_root,
                    backup_directory,
                    basename(filename)
                )
                if not self.is_dry_run:
                    os.link(filename, target_filename)

    def effect_deletions(self):
        """ Deletes the files which have been listed for deletion based on the
            backup_plan """
        backup_plan = self.backup_plan
        LOG.debug("Results: %s", json.dumps(
            backup_plan,
            sort_keys=True,
            indent=4,
            default=lambda x: list(x) if isinstance(x, set) else "Skip"))
        LOG.debug("Handling deletions")
        # Collect all files to delete into a set (if a file was cycled out
        # of both daily AND a monthly then we will attempt to delete twice).
        files_to_delete = set()
        for backup_directory in map(lambda x: x[0], self.__time_buckets):
            files_to_delete = files_to_delete | \
                    backup_plan[backup_directory]["files_to_delete"]
        # Delete if we are not a dry run.
        for filename in sorted(files_to_delete):
            LOG.debug("Deleting %s", filename)
            if not self.is_dry_run:
                os.remove(filename)

    def plan_promotions_and_deletions(self):
        """Generates a backup plan by walking through the time_buckets
           ordered by frequency and scanning the files. The
           time buckets must be ordered by decreasing grandularity
           (e.g. yearly first, daily last)"""
        backup_plan = self.backup_plan
        # Represents all of the time_buckets we've visited so far (as
        # we need to go back through their results)
        processed = []

        # The filename pattern for the files to consider
        pattern = fnmatch.translate(self.pattern)

        # Scan through each backup directory listed
        for backup_directory, config in self.__time_buckets:
            # Initialize the results for the current directory
            files_to_delete = set()
            files_to_promote = []
            results = {
                "files_to_keep": [],
                "files_to_delete": files_to_delete,
                "files_to_promote": files_to_promote
            }
            backup_plan[backup_directory] = results
            LOG.info("Processing %s", backup_directory)

            # Walk through all of the files in the target backup directory
            backup_abs_dir = join(self.backup_root, backup_directory)
            for (dirpath, _, filenames) in os.walk(backup_abs_dir):
                # Note: ascending by default, so oldest files first.
                sorted_file_abs_paths = sorted(map(
                    partial(lambda x, dirpath_part: join(dirpath_part, x),
                            dirpath_part=dirpath),
                    filter(lambda x: re.match(pattern, x), filenames)),
                    key=getmtime)

                self.process_files(results, sorted_file_abs_paths,
                                   config, processed)

            # Now that we've processed the directory, we want to "save" any
            # files marked for deletion younger than
            # num_files_to_keep * timeunit
            self.resurrect_young_files(
                    results["files_to_keep"],
                    files_to_delete,
                    config)
            # Resort the list by modification time in case it was modified.
            results["files_to_keep"] = \
                sorted(results["files_to_keep"], key=os.path.getmtime)

            processed.append([backup_directory, config])

    def resurrect_young_files(self, files_to_keep, files_to_delete, config):
        """ Scans the files_to_delete for files which are within the grace period
            and ressurrects them. """
        if files_to_keep:
            safe_duration = config["frequency"] * \
                    (config["num_files_to_keep"] - 1)
            safe_after_date = self.__get_mod_time(files_to_keep[-1]) - safe_duration
            for filename in list(files_to_delete):
                if self.__get_mod_time(filename) > safe_after_date:
                    files_to_delete.remove(filename)
                    files_to_keep.append(filename)
                    LOG.debug("Ressurrected file %s because it's "
                              "too young to die.", filename)

    def process_files(self, results, sorted_abs_paths, config, processed):
        """ Runs through the files provided and processes them, adding them to the
            the appropriate lists (keep, promote, delete) """
        backup_plan = self.backup_plan
        for filename in sorted_abs_paths:
            self.process_file(results, config, filename)
            for promotion_dir, promotion_target_config in processed:
                self.process_file(backup_plan[promotion_dir],
                             promotion_target_config, filename, True)

    def process_file(self, backup_results, backup_config, filename, promotion=False):
        """ Processes a file by adding it to the appropriate list (files_to_keep,
            files_to_promote, files_to_delete) """
        # Note: This logic assumes and requires that "files_to_keep" is pre-sorted
        #       in chronological order
        files_to_keep = backup_results["files_to_keep"]
        files_to_delete = backup_results["files_to_delete"]
        files_to_keep = backup_results["files_to_keep"]
        files_to_promote = backup_results["files_to_promote"]
        freq = backup_config["frequency"]
        if not files_to_keep or \
                self.__get_mod_time(filename) >= self.__get_mod_time(files_to_keep[-1]) + freq:
            if len(files_to_keep) >= backup_config["num_files_to_keep"]:
                reject_file = files_to_keep.pop(0)
                files_to_delete.add(reject_file)
                if reject_file in files_to_promote:
                    files_to_promote.remove(reject_file)
            files_to_keep.append(filename)
            if promotion:
                files_to_promote.append(filename)
        elif not promotion:
            files_to_delete.add(filename)


    def rotate_backups(self):
        """ Creates a plan, then affects promotions and deletions on it. """
        if not os.path.exists(self.backup_root):
            raise BackupRootFolderMissingException(self.backup_root)

        for item in self.__time_buckets.copy():
            dir_name = item[0]
            full_path = os.path.join(self.backup_root, dir_name)
            if not os.path.exists(full_path):
                logging.warning("The backup directory %s is missing, "
                                "removing the time bucket.", full_path)
                self.__time_buckets.remove(item)
                os.mkdir(full_path)

        self.plan_promotions_and_deletions()
        self.effect_promotions()
        self.effect_deletions()
